{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, StructType, StructField, TimestampType, DateType, ArrayType, LongType\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-8e81-m.us-east4-b.c.cse6242-329416.internal:37195\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(loc):\n",
    "    df = sqlContext.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"timestampFormat\", \"MM/dd/yyyy HH:mm:SS\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(loc)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def writeOutput(df, loc):\n",
    "    df.repartition(1).write.option(\"header\", \"true\").csv(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+--------+------+----------+----------+--------+--------+---+-------+---+----+----------+--------+---------+-------+----+----+\n",
      "|               DATE|           NEW_DATE|    DATE_STRING|AVG_WIND|PRECIP|SNOW_TOTAL|SNOW_DEPTH|MAX_TEMP|MIN_TEMP|FOG|THUNDER|ICE|HAIL|HAZE_SMOKE|BLW_SNOW|HIGH_WIND|DRIZZLE|RAIN|SNOW|\n",
      "+-------------------+-------------------+---------------+--------+------+----------+----------+--------+--------+---+-------+---+----+----------+--------+---------+-------+----+----+\n",
      "|2010-01-01 00:00:00|2010-01-01 00:00:00|01 January 2010|    3.36|  0.03|       0.0|       0.0|      40|      33|Yes|     No| No|  No|        No|      No|       No|     No| Yes| Yes|\n",
      "|2010-01-02 00:00:00|2010-01-02 00:00:00|02 January 2010|   12.53|  0.02|       0.2|       0.0|      34|      17|Yes|     No| No|  No|       Yes|      No|       No|     No| Yes| Yes|\n",
      "|2010-01-03 00:00:00|2010-01-03 00:00:00|03 January 2010|   14.32|   0.0|       0.0|       0.0|      22|      17| No|     No| No|  No|       Yes|      No|       No|     No|  No| Yes|\n",
      "|2010-01-04 00:00:00|2010-01-04 00:00:00|04 January 2010|   10.74|   0.0|       0.0|       0.0|      30|      19| No|     No| No|  No|        No|      No|       No|     No|  No|  No|\n",
      "|2010-01-05 00:00:00|2010-01-05 00:00:00|05 January 2010|    9.84|   0.0|       0.0|       0.0|      30|      20| No|     No| No|  No|        No|      No|       No|     No|  No| Yes|\n",
      "+-------------------+-------------------+---------------+--------+------+----------+----------+--------+--------+---+-------+---+----+----------+--------+---------+-------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucket = 'gs://project_ggallagher6/' #201410-citibike-tripdata.csv'\n",
    "\n",
    "weatherLoc = bucket+'wx_data_noaa_cleaned.csv'\n",
    "df = loadData(weatherLoc)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+----------+--------+--------+---+-------+---+----+----------+--------+---------+-------+----+----+-----------+-----------+\n",
      "|AVG_WIND|PRECIP|SNOW_TOTAL|SNOW_DEPTH|MAX_TEMP|MIN_TEMP|FOG|THUNDER|ICE|HAIL|HAZE_SMOKE|BLW_SNOW|HIGH_WIND|DRIZZLE|RAIN|SNOW|weatherHour|weatherDate|\n",
      "+--------+------+----------+----------+--------+--------+---+-------+---+----+----------+--------+---------+-------+----+----+-----------+-----------+\n",
      "|    3.36|  0.03|       0.0|       0.0|      40|      33|Yes|     No| No|  No|        No|      No|       No|     No| Yes| Yes|          0| 2010-01-01|\n",
      "|   12.53|  0.02|       0.2|       0.0|      34|      17|Yes|     No| No|  No|       Yes|      No|       No|     No| Yes| Yes|          0| 2010-01-02|\n",
      "|   14.32|   0.0|       0.0|       0.0|      22|      17| No|     No| No|  No|       Yes|      No|       No|     No|  No| Yes|          0| 2010-01-03|\n",
      "|   10.74|   0.0|       0.0|       0.0|      30|      19| No|     No| No|  No|        No|      No|       No|     No|  No|  No|          0| 2010-01-04|\n",
      "|    9.84|   0.0|       0.0|       0.0|      30|      20| No|     No| No|  No|        No|      No|       No|     No|  No| Yes|          0| 2010-01-05|\n",
      "+--------+------+----------+----------+--------+--------+---+-------+---+----+----------+--------+---------+-------+----+----+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather = df.withColumn('weatherHour', hour(df.DATE)).withColumn('weatherDate', to_date(df.DATE))\n",
    "#weather.printSchema()\n",
    "weatherMin = weather.drop('DATE', 'NEW_DATE', 'DATE_STRING')\n",
    "weatherMin.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadData(bucket+'2014_demand.csv')\n",
    "stations = loadData(bucket+'2014_stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+-------------------+----+----------------+-----+\n",
      "|               date|hour|start_station_id|count|\n",
      "+-------------------+----+----------------+-----+\n",
      "|2014-01-01 00:00:00|   2|             157|    1|\n",
      "|2014-01-01 00:00:00|   4|             439|    1|\n",
      "+-------------------+----+----------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stationList = list(stations.select('station_id').toPandas()['station_id'])\n",
    "print(len(stationList))\n",
    "\n",
    "data.printSchema()\n",
    "data.show(2)\n",
    "\n",
    "#data2016Clean = data2016.withColumn('date', to_date(data2016.date))\n",
    "#test = data2016Clean.filter(data2016Clean.start_station_id == 492)\n",
    "#test2 = test.groupBy('date','hour','start_station_id').count()\n",
    "\n",
    "#https://stackoverflow.com/questions/46709285/filling-missing-dates-in-spark-dataframe-column\n",
    "#test2 = test2.orderBy(test2.date, test2.hour, ascending=True)\n",
    "#test2.show(5)\n",
    "#data2015Clean.show(5)\n",
    "#data2015Clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test2.show(5)\n",
    "for i in range(len(stationList)):\n",
    "    station = stationList[i]\n",
    "    stationDemand = data.filter(data.start_station_id == station)\n",
    "    stationDemandDF = stationDemand.toPandas()\n",
    "    stationDemandDF['date'] = pd.to_datetime(stationDemandDF['date'])\n",
    "    stationDemandDF = stationDemandDF.astype({'hour':'int64', 'start_station_id': 'int64'})\n",
    "    r = pd.date_range(start='2014-01-01', end=stationDemandDF.date.max())\n",
    "    #print(r)\n",
    "    h = []\n",
    "    for i in range(24):\n",
    "        h.append(i)\n",
    "    \n",
    "    iterables = [r, h]\n",
    "\n",
    "    new_index = pd.MultiIndex.from_product(iterables, names=[\"d\", \"h\"])\n",
    "    #print(bob)\n",
    "    df = stationDemandDF.set_index(['date','hour']).reindex(new_index).fillna(0).rename_axis(['date', 'hour']).reset_index()\n",
    "    df['start_station_id'] = stationList[0]\n",
    "    tst = tst.astype({'count':'int64'})\n",
    "    \n",
    "    weatherDF = weatherMin.toPandas()\n",
    "    weatherDF['weatherDate'] = pd.to_datetime(weatherDF['weatherDate'])\n",
    "    bob = df.merge(weatherDF, how='left', left_on='date', right_on='weatherDate')\n",
    "    bob.to_csv(bucket+'stationDemand/2014_'+str(station)+'.csv', index=False)\n",
    "    #test = data2019Clean.join(weatherMin,(data2019Clean.date == weatherMin.weatherDate), how='left')\n",
    "    \n",
    "    \n",
    "#test = data.filter(data.start_station_id == stationList[0])\n",
    "#test = test.orderBy(test.date, test.hour, ascending=True)\n",
    "#test.show(5)\n",
    "if False:\n",
    "    testDF = test.toPandas()\n",
    "    testDF['date'] = pd.to_datetime(testDF['date'])\n",
    "    testDF = testDF.astype({'hour':'int64', 'start_station_id': 'int64'})\n",
    "    r = pd.date_range(start='2014-01-01', end=testDF.date.max())\n",
    "    #print(r)\n",
    "    h = []\n",
    "    for i in range(24):\n",
    "        h.append(i)\n",
    "#print(h)\n",
    "#testDF.set_index('date').reindex(r).fillna(0.0).rename_axis('date').reset_index()\n",
    "#df_filled = df.groupby('date', 'hour').apply(lambda group: group.fillna(value=0))\n",
    "#print(testDF)h\n",
    "#print(r)\n",
    "#print(testDF)\n",
    "\n",
    "    iterables = [r, h]\n",
    "\n",
    "    bob = pd.MultiIndex.from_product(iterables, names=[\"d\", \"h\"])\n",
    "    #print(bob)\n",
    "    tst = testDF.set_index(['date','hour']).reindex(bob).fillna(0).rename_axis(['date', 'hour']).reset_index()\n",
    "    tst['start_station_id'] = stationList[0]\n",
    "    tst = tst.astype({'count':'int64'})\n",
    "    display(tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b452e2979754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#tst.to_csv(bucket+'tacos/bob.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#bucket+'test/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstationList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#stationList = stationList.sort()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstationList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#print(testDF.info())\n",
    "#tim = testDF.copy(deep=True)\n",
    "#tim['date'] = pd.to_datetime(tim['date'])\n",
    "#tim = tim.astype({'hour':'int64', 'start_station_id': 'int64'})\n",
    "#print(tim.info())\n",
    "#df.astype({'col1': 'int32'})\n",
    "#print(tim)\n",
    "#print(tst.info())\n",
    "#tst.to_csv(bucket+'tacos/bob.csv')\n",
    "#bucket+'test/'\n",
    "print(str(len(stationList)))\n",
    "#stationList = stationList.sort()\n",
    "print(stationList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4307 entries, 0 to 4306\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   AVG_WIND     4307 non-null   object        \n",
      " 1   PRECIP       4307 non-null   float64       \n",
      " 2   SNOW_TOTAL   4307 non-null   float64       \n",
      " 3   SNOW_DEPTH   4306 non-null   float64       \n",
      " 4   MAX_TEMP     4307 non-null   int32         \n",
      " 5   MIN_TEMP     4307 non-null   int32         \n",
      " 6   FOG          4307 non-null   object        \n",
      " 7   THUNDER      4307 non-null   object        \n",
      " 8   ICE          4307 non-null   object        \n",
      " 9   HAIL         4307 non-null   object        \n",
      " 10  HAZE_SMOKE   4307 non-null   object        \n",
      " 11  BLW_SNOW     4307 non-null   object        \n",
      " 12  HIGH_WIND    4307 non-null   object        \n",
      " 13  DRIZZLE      4307 non-null   object        \n",
      " 14  RAIN         4307 non-null   object        \n",
      " 15  SNOW         4307 non-null   object        \n",
      " 16  weatherHour  4307 non-null   int32         \n",
      " 17  weatherDate  4307 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(3), int32(3), object(11)\n",
      "memory usage: 555.3+ KB\n",
      "None\n",
      "           date  hour  start_station_id  count AVG_WIND  PRECIP  SNOW_TOTAL  \\\n",
      "0    2016-01-01     0               519      1     7.61     0.0         0.0   \n",
      "1    2016-01-01     1               519      2     7.61     0.0         0.0   \n",
      "2    2016-01-01     2               519      0     7.61     0.0         0.0   \n",
      "3    2016-01-01     3               519      0     7.61     0.0         0.0   \n",
      "4    2016-01-01     4               519      0     7.61     0.0         0.0   \n",
      "...         ...   ...               ...    ...      ...     ...         ...   \n",
      "8779 2016-12-31    19               519      0     6.93     0.0         0.0   \n",
      "8780 2016-12-31    20               519      1     6.93     0.0         0.0   \n",
      "8781 2016-12-31    21               519      3     6.93     0.0         0.0   \n",
      "8782 2016-12-31    22               519      1     6.93     0.0         0.0   \n",
      "8783 2016-12-31    23               519      1     6.93     0.0         0.0   \n",
      "\n",
      "      SNOW_DEPTH  MAX_TEMP  MIN_TEMP  ... ICE HAIL HAZE_SMOKE BLW_SNOW  \\\n",
      "0            0.0        42        34  ...  No   No         No       No   \n",
      "1            0.0        42        34  ...  No   No         No       No   \n",
      "2            0.0        42        34  ...  No   No         No       No   \n",
      "3            0.0        42        34  ...  No   No         No       No   \n",
      "4            0.0        42        34  ...  No   No         No       No   \n",
      "...          ...       ...       ...  ...  ..  ...        ...      ...   \n",
      "8779         0.0        44        31  ...  No   No         No       No   \n",
      "8780         0.0        44        31  ...  No   No         No       No   \n",
      "8781         0.0        44        31  ...  No   No         No       No   \n",
      "8782         0.0        44        31  ...  No   No         No       No   \n",
      "8783         0.0        44        31  ...  No   No         No       No   \n",
      "\n",
      "     HIGH_WIND DRIZZLE RAIN SNOW weatherHour weatherDate  \n",
      "0           No      No   No   No           0  2016-01-01  \n",
      "1           No      No   No   No           0  2016-01-01  \n",
      "2           No      No   No   No           0  2016-01-01  \n",
      "3           No      No   No   No           0  2016-01-01  \n",
      "4           No      No   No   No           0  2016-01-01  \n",
      "...        ...     ...  ...  ...         ...         ...  \n",
      "8779        No      No   No   No           0  2016-12-31  \n",
      "8780        No      No   No   No           0  2016-12-31  \n",
      "8781        No      No   No   No           0  2016-12-31  \n",
      "8782        No      No   No   No           0  2016-12-31  \n",
      "8783        No      No   No   No           0  2016-12-31  \n",
      "\n",
      "[8784 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "weatherDF = weatherMin.toPandas()\n",
    "weatherDF['weatherDate'] = pd.to_datetime(weatherDF['weatherDate'])\n",
    "bob = tst.merge(weatherDF, how='left', left_on='date', right_on='weatherDate')\n",
    "print(weatherDF.info())\n",
    "print(bob)\n",
    "bob.to_csv(bucket+'2016_492.csv', index=False)\n",
    "#test = data2019Clean.join(weatherMin,(data2019Clean.date == weatherMin.weatherDate), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "2015-04-01\n",
      "trying 2015-01-01 and 0\n",
      "trying 2015-01-01 and 1\n",
      "trying 2015-01-01 and 2\n",
      "trying 2015-01-01 and 3\n",
      "trying 2015-01-01 and 4\n",
      "trying 2015-01-01 and 5\n",
      "trying 2015-01-01 and 6\n",
      "trying 2015-01-01 and 7\n",
      "trying 2015-01-01 and 8\n",
      "trying 2015-01-01 and 9\n",
      "trying 2015-01-01 and 10\n",
      "trying 2015-01-01 and 11\n",
      "trying 2015-01-01 and 12\n",
      "trying 2015-01-01 and 13\n",
      "trying 2015-01-01 and 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-89602054aea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trying \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mday_delta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" and \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtest3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mday_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtest3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mnewDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mday_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m492\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtest2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \"\"\"\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test2.printSchema()\n",
    "bob = test2.collect()[0][0]\n",
    "print(bob)\n",
    "#datetime.datetime.strptime('24052010', \"%d%m%Y\").date()\n",
    "# The size of each step in days https://stackoverflow.com/questions/46709285/filling-missing-dates-in-spark-dataframe-column\n",
    "\n",
    "day_delta = datetime.timedelta(days=1)\n",
    "\n",
    "start_date = datetime.datetime.strptime('01012015', \"%d%m%Y\").date()\n",
    "end_date = start_date + 365*day_delta\n",
    "columns = ['date', 'hour', 'start_station_id', 'count']\n",
    "\n",
    "for i in range((end_date - start_date).days):\n",
    "    for j in range(24):\n",
    "        #    print(start_date + i*day_delta)\n",
    "        print(\"trying \" + str(start_date + i*day_delta) + \" and \" + str(j))\n",
    "        test3 = test2.filter((test2.date.cast(StringType()) == str(start_date + i*day_delta)) & (test2.hour == j))\n",
    "        if test3.count() == 0:\n",
    "            newDF = spark.createDataFrame([(start_date + i*day_delta, j, 492, 0)], columns)\n",
    "            test2 = test2.union(newDF)\n",
    "            \n",
    "print(str(test2.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------------+--------------+-----+--------+------+----------+----------+--------+--------+---+-------+---+----+----------+--------+---------+-------+----+----+-----------+-----------+\n",
      "|      date|hour|start_station_id|end_station_id|count|AVG_WIND|PRECIP|SNOW_TOTAL|SNOW_DEPTH|MAX_TEMP|MIN_TEMP|FOG|THUNDER|ICE|HAIL|HAZE_SMOKE|BLW_SNOW|HIGH_WIND|DRIZZLE|RAIN|SNOW|weatherHour|weatherDate|\n",
      "+----------+----+----------------+--------------+-----+--------+------+----------+----------+--------+--------+---+-------+---+----+----------+--------+---------+-------+----+----+-----------+-----------+\n",
      "|2019-01-01|   1|             326|           301|    1| Unknown|  0.06|       0.0|       0.0|      58|      39|Yes|     No| No|  No|        No|      No|       No|     No|  No|  No|          0| 2019-01-01|\n",
      "|2019-01-01|   2|             447|           461|    1| Unknown|  0.06|       0.0|       0.0|      58|      39|Yes|     No| No|  No|        No|      No|       No|     No|  No|  No|          0| 2019-01-01|\n",
      "+----------+----+----------------+--------------+-----+--------+------+----------+----------+--------+--------+---+-------+---+----+----------+--------+---------+-------+----+----+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bd61cef4b22d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weatherDate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weatherHour'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwriteOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'test/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-368e32cff8fa>\u001b[0m in \u001b[0;36mwriteOutput\u001b[0;34m(df, loc)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwriteOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)\u001b[0m\n\u001b[1;32m    933\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                        encoding=encoding, emptyValue=emptyValue)\n\u001b[0;32m--> 935\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = data2019Clean.join(weatherMin,(data2019Clean.date == weatherMin.weatherDate), how='left')\n",
    "test.show(2)\n",
    "test.drop('weatherDate', 'weatherHour')\n",
    "writeOutput(test, bucket+'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7185729\n",
      "7185729\n"
     ]
    }
   ],
   "source": [
    "print(data2014Clean.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2014, 2020):\n",
    "    data = loadData(bucket+'cleaned/' + str(i) +'.csv')\n",
    "    \n",
    "    out = data.join(weatherMin,(data.date == weatherMin.weatherDate), how='left')\n",
    "    out = out.drop('weatherDate', 'weatherHour')\n",
    "\n",
    "    writeOutput(out, bucket+\"cleanWeather2/\" + str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}